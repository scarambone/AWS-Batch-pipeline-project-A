
from pyspark.sql import SparkSession
from pyspark.sql.functions import when, col

spark = SparkSession.builder.appName("SalesDataTransformationDebug").getOrCreate()

input_path = "s3://alexandre-batch-pipeline/raw/sales_data.csv"
output_path = "s3://alexandre-batch-pipeline/processed/"

try:
    print("ğŸ“¥ Lendo arquivo CSV do S3...")
    df = spark.read.option("header", "true").csv(input_path)
    print("âœ… Schema do DataFrame:")
    df.printSchema()

    print("ğŸ” Primeiras linhas do dataset:")
    df.show(5)

    print("ğŸ”„ Convertendo coluna 'total_amount' para double...")
    df = df.withColumn("total_amount", col("total_amount").cast("double"))

    print("ğŸ“Š Adicionando coluna de classificaÃ§Ã£o de valor...")
    df = df.withColumn(
        "amount_category",
        when(col("total_amount") >= 300, "high").otherwise("low")
    )

    print("ğŸ” Filtrando vendas com total_amount >= 300...")
    df_filtered = df.filter(col("total_amount") >= 300)

    print("ğŸ’¾ Gravando saÃ­da como Parquet no S3...")
    df_filtered.write.mode("overwrite").parquet(output_path)

    print("âœ… Job finalizado com sucesso!")

except Exception as e:
    print("âŒ Erro durante a execuÃ§Ã£o do job:", str(e))

spark.stop()
