
# ğŸ“¦ Project: Batch Data Pipeline on AWS with PySpark

## ğŸ“Œ Overview

This project showcases a complete batch data processing pipeline built entirely on **AWS**, using **Apache Spark on Amazon EMR** for scalable transformations. It simulates a realistic enterprise scenario in which raw sales data is ingested, transformed, stored in optimized formats, cataloged, and queried â€” all within the AWS ecosystem.

## ğŸ¯ Goals

- Ingest raw CSV data from **Amazon S3**
- Process and transform the data using **PySpark on Amazon EMR**
- Filter high-value transactions (â‰¥ 300)
- Add a classification column (`high` or `low`) based on purchase amount
- Store the output in **Parquet format** back to S3
- Catalog the transformed dataset with **AWS Glue**
- Enable serverless SQL analytics using **Amazon Athena**

## ğŸ§± Architecture

```
S3 (raw data)
   â†“
Amazon EMR (PySpark job)
   â†“
S3 (processed Parquet files)
   â†“
AWS Glue Crawler â†’ Glue Catalog
   â†“
Amazon Athena (SQL queries)
```

## ğŸ› ï¸ Technologies Used

- **Amazon S3** â€“ Data lake storage
- **Amazon EMR** â€“ Managed Spark cluster
- **PySpark** â€“ Batch transformation logic
- **AWS Glue** â€“ Schema discovery and data cataloging
- **Amazon Athena** â€“ Serverless querying engine

## ğŸ“Š Dataset Example

Sample sales dataset containing:
- `order_id`, `customer_id`, `order_date`, `total_amount`
- Derived column: `amount_category` (`high` if â‰¥ 300, else `low`)

## âœ… Project Outcome

- Built a functional batch ETL pipeline using AWS services
- Processed raw CSV files and outputted clean Parquet datasets
- Registered processed data in the AWS Glue Catalog
- Verified the pipeline by querying transformed data using Athena
